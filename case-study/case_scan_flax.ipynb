{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import jax\n","from typing import Any, Callable, Sequence\n","from jax import random, numpy as jnp\n","import flax\n","from flax import linen as nn"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["model = nn.Dense(features=5)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["{'params': {'bias': (5,), 'kernel': (10, 5)}}"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["key1, key2 = random.split(random.key(0))\n","x = random.normal(key1, (10,))\n","params = model.init(key2, x)\n","jax.tree_util.tree_map(lambda x: x.shape, params)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x shape: (20, 10) ; y shape: (20, 5)\n"]}],"source":["n_samples = 20\n","x_dim = 10\n","y_dim = 5\n","\n","key = random.key(0)\n","k1, k2 = random.split(key)\n","W = random.normal(k1, (x_dim, y_dim))\n","b = random.normal(k2, (y_dim,))\n","true_params = flax.core.freeze({'params': {'bias': b, 'kernel': W}})\n","\n","key_sample, key_noise = random.split(k1)\n","x_samples = random.normal(key_sample, (n_samples, x_dim))\n","y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise,(n_samples, y_dim))\n","print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["@jax.jit\n","def mse(params, x_batched, y_batched):\n","  def squared_error(x, y):\n","    pred = model.apply(params, x)\n","    return jnp.inner(y-pred, y-pred) / 2.0\n","  return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss for \"true\" W,b:  0.023639796\n","Loss step 0:  35.343876\n","Loss step 10:  0.51505077\n","Loss step 20:  0.11404524\n","Loss step 30:  0.039395202\n","Loss step 40:  0.01994018\n","Loss step 50:  0.014217627\n","Loss step 60:  0.012428728\n","Loss step 70:  0.011851473\n","Loss step 80:  0.011662121\n","Loss step 90:  0.011599523\n","Loss step 100:  0.01157873\n"]}],"source":["learning_rate = 0.3\n","print('Loss for \"true\" W,b: ', mse(true_params, x_samples, y_samples))\n","loss_grad_fn = jax.value_and_grad(mse)\n","\n","@jax.jit\n","def update_params(params, learning_rate, grads):\n","  params = jax.tree_util.tree_map(\n","      lambda p, g: p - learning_rate * g, params, grads)\n","  return params\n","\n","for i in range(101):\n","  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n","  params = update_params(params, learning_rate, grads)\n","  if i % 10 == 0:\n","    print(f'Loss step {i}: ', loss_val)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss step 0: 35.343875885009766\n","Loss step 10: 0.5150507688522339\n","Loss step 20: 0.11404523998498917\n","Loss step 30: 0.039395201951265335\n","Loss step 40: 0.019940180703997612\n","Loss step 50: 0.014217627234756947\n","Loss step 60: 0.012428727932274342\n","Loss step 70: 0.011851472780108452\n","Loss step 80: 0.011662120930850506\n","Loss step 90: 0.011599523015320301\n","Loss step 100: 0.011578730307519436\n"]}],"source":["params = model.init(key2, x)\n","\n","def rollout(carry, x):\n","    params, step = carry\n","    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n","    params = update_params(params, learning_rate, grads)\n","\n","    def print_loss_fn():\n","        jax.debug.print('Loss step {i}: {loss}', i=step, loss=loss_val)\n","        return loss_val\n","    \n","    loss_val = jax.lax.cond(step % 10 == 0, lambda _: print_loss_fn(), lambda _: loss_val, operand=None)\n","    return (params, step+1), loss_val\n","\n","_, loss_vals = jax.lax.scan(rollout, (params, 0), None, 101)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["params = model.init(key2, x)\n","\n","def rollout(carry, x):\n","    (params, step, loss_history) = carry\n","    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n","    params = update_params(params, learning_rate, grads)\n","\n","    def store_loss_fn():\n","        return loss_history.at[step // 10].set(loss_val)\n","    \n","    loss_history = jax.lax.cond(step % 10 == 0, lambda _: store_loss_fn(), lambda _: loss_history, operand=None)\n","    return (params, step+1, loss_history), loss_val\n","#                                            f         init                                 xs    length\n","(_, _, final_loss_history), _ = jax.lax.scan(rollout, (params, 0, jnp.zeros((101 // 10,))), None, 101)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["Array([3.5343876e+01, 5.1505077e-01, 1.1404524e-01, 3.9395202e-02,\n","       1.9940181e-02, 1.4217627e-02, 1.2428728e-02, 1.1851473e-02,\n","       1.1662121e-02, 1.1599523e-02], dtype=float32)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["final_loss_history"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":2}
